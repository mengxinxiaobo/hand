"""
web_app.py - ä¼˜åŒ–ç‰ˆ
åŒ…å«é«˜çº§å¹³æ»‘å¤„ç†å’Œä¸¥æ ¼ç½®ä¿¡åº¦è¿‡æ»¤ï¼Œå¯¹é½ main.py çš„è¯†åˆ«ä½“éªŒ
"""
import os
import cv2
import time
import numpy as np
import traceback
import collections
from flask import Flask, render_template, Response, jsonify, request
import datetime

# å¼•å…¥ä½ çš„æ ¸å¿ƒæ¨¡å— (ç¡®ä¿è¿™ä¸¤ä¸ªæ–‡ä»¶åœ¨åŒçº§ç›®å½•)
from hand_landmarks import HandLandmarkDetector
from gesture_classifier import GestureClassifier

app = Flask(__name__)

# ===========================
# 1. åˆå§‹åŒ–æ ¸å¿ƒæ¨¡å‹
# ===========================
detector = HandLandmarkDetector()
classifier = GestureClassifier()

# ===========================
# 2. å…¨å±€çŠ¶æ€ç®¡ç† (æ ¸å¿ƒ)
# ===========================
class AppState:
    def __init__(self):
        self.mode = 'recognition'       # 'recognition' æˆ– 'learning'

        # --- è¯†åˆ«æ ¸å¿ƒå‚æ•° ---
        self.confidence_threshold = 0.6 # é»˜è®¤é˜ˆå€¼ï¼Œä½äºæ­¤å€¼ä¸æ˜¾ç¤º
        self.smooth_window = 2          # å¹³æ»‘çª—å£å¤§å° (5å¸§)

        # --- å®æ—¶çŠ¶æ€ ---
        self.last_prediction = None     # æœ€ç»ˆè¾“å‡ºç»™å‰ç«¯çš„é¢„æµ‹ç»“æœ
        self.last_confidence = 0.0      # æœ€ç»ˆç½®ä¿¡åº¦
        self.hand_detected = False      # æ˜¯å¦æ£€æµ‹åˆ°æ‰‹

        # --- å†å²ä¸ç»Ÿè®¡ ---
        # ç”¨äºå¹³æ»‘çš„é˜Ÿåˆ— (å­˜å‚¨æœ€è¿‘5å¸§çš„åŸå§‹é¢„æµ‹)
        self.raw_predictions_queue = collections.deque(maxlen=self.smooth_window)
        self.prediction_history = []    # å‘é€ç»™å‰ç«¯çš„å†å²åˆ—è¡¨
        self.gesture_statistics = {}    # ç»Ÿè®¡æ•°æ®
        self.performance_metrics = {
            'frame_rate': 0,
            'detection_time': 0,
            'classification_time': 0
        }

        # --- æ§åˆ¶å¼€å…³ ---
        self.is_running = False         # æ˜¯å¦å¯åŠ¨æ‘„åƒå¤´
        self.show_landmarks = True      # æ˜¯å¦ç»˜åˆ¶éª¨æ¶
        self.is_recording = False       # æ˜¯å¦è®°å½•
        self.current_gesture = None     # å­¦ä¹ æ¨¡å¼ä¸‹çš„ç›®æ ‡æ‰‹åŠ¿
        self.demo_gesture = None        # æ¼”ç¤ºæ¨¡å¼æ‰‹åŠ¿

state = AppState()

# ===========================
# 3. æ ¸å¿ƒé€»è¾‘ï¼šè§†é¢‘æµå¤„ç†
# ===========================
def generate_frames():
    import time

    while not state.is_running:
        frame = np.zeros((480, 640, 3), dtype=np.uint8)
        ret, buffer = cv2.imencode('.jpg', frame)
        yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')
        time.sleep(0.5)

    # å°è¯•å¼€å¯ DSHOW åŠ é€Ÿ
    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
    if not cap.isOpened():
        cap = cv2.VideoCapture(0)

    # ğŸ”´ è®¾ç½®æ‘„åƒå¤´åˆ†è¾¨ç‡ (é™ä½åˆ†è¾¨ç‡ = æå¤§æé€Ÿ)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)

    frame_count = 0
    start_time = time.time()

    # ğŸ”´ è·³å¸§è®¡æ•°å™¨
    process_interval = 2  # æ¯ 2 å¸§æ£€æµ‹ä¸€æ¬¡ (1è¡¨ç¤ºæ¯å¸§éƒ½æ£€ï¼Œ2è¡¨ç¤ºéš”ä¸€å¸§ï¼Œ3è¡¨ç¤ºéš”ä¸¤å¸§)
    current_frame_index = 0
    last_annotated_frame = None

    try:
        while state.is_running:
            frame_count += 1
            current_frame_index += 1

            # è®¡ç®—FPS
            if frame_count % 15 == 0:
                elapsed = time.time() - start_time
                if elapsed > 0:
                    state.performance_metrics['frame_rate'] = round(15 / elapsed, 1)
                start_time = time.time()
                frame_count = 0

            success, frame = cap.read()
            if not success:
                break

            frame = cv2.flip(frame, 1)

            # ğŸ”´ åªæœ‰æ»¡è¶³é—´éš”æ—¶ï¼Œæ‰è¿›è¡Œé‡å‹è®¡ç®— (AIè¯†åˆ«)
            if current_frame_index % process_interval == 0:

                # 1. æ£€æµ‹
                t1 = time.time()
                landmarks, annotated_frame = detector.detect(frame)
                state.performance_metrics['detection_time'] = round((time.time() - t1) * 1000, 2)
                state.hand_detected = (landmarks is not None)

                # ç¼“å­˜è¿™ä¸€å¸§çš„ç”»é¢ï¼Œä¸‹ä¸€å¸§ç›´æ¥ç”¨ï¼Œçœç®—åŠ›
                last_annotated_frame = annotated_frame

                # 2. è¯†åˆ«
                current_raw_pred = None
                current_raw_conf = 0.0

                if landmarks is not None and len(landmarks) > 0:
                    features = detector.extract_features(landmarks)
                    if features is not None and len(features) > 0:
                        t2 = time.time()
                        raw_pred, raw_conf = classifier.predict(features)
                        state.performance_metrics['classification_time'] = round((time.time() - t2) * 1000, 2)
                        current_raw_pred = raw_pred
                        current_raw_conf = raw_conf

                # 3. æ•°æ®å¹³æ»‘
                if current_raw_pred:
                    state.raw_predictions_queue.append((current_raw_pred, current_raw_conf))
                else:
                    # æ³¨æ„ï¼šè·³å¸§æ—¶ä¸è¦æ¸…ç©ºé˜Ÿåˆ—ï¼Œåªæœ‰çœŸæ­£æ²¡æ£€æµ‹åˆ°æ‰æ¸…ç©º
                    if landmarks is None:
                        state.raw_predictions_queue.clear()
                        state.last_prediction = None

                if len(state.raw_predictions_queue) >= min(3, state.smooth_window):
                    gestures = [p[0] for p in state.raw_predictions_queue]
                    most_common_gesture = max(set(gestures), key=gestures.count)
                    avg_confidence = np.mean([c for g, c in state.raw_predictions_queue if g == most_common_gesture])

                    if avg_confidence >= state.confidence_threshold:
                        state.last_prediction = most_common_gesture
                        state.last_confidence = float(avg_confidence)
                        state.gesture_statistics[most_common_gesture] = state.gesture_statistics.get(
                            most_common_gesture, 0) + 1

                        if not state.prediction_history or state.prediction_history[-1][0] != most_common_gesture:
                            state.prediction_history.append((most_common_gesture, float(avg_confidence)))
                            if len(state.prediction_history) > 20:
                                state.prediction_history.pop(0)
                    else:
                        state.last_prediction = None
                        state.last_confidence = float(avg_confidence)

            else:
                # ğŸ”´ è·³è¿‡çš„å¸§ï¼šç›´æ¥ä½¿ç”¨ä¸Šä¸€å¸§çš„ç”»é¢ (æˆ–è€…åªæ˜¾ç¤ºåŸå›¾)
                if last_annotated_frame is not None:
                    annotated_frame = last_annotated_frame
                else:
                    annotated_frame = frame

            # æ˜¾ç¤º
            final_frame = annotated_frame if state.show_landmarks else frame
            ret, buffer = cv2.imencode('.jpg', final_frame)
            yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')

            # ğŸ”´ æé€Ÿæ¨¡å¼ä¸‹ï¼Œsleepå¯ä»¥å‡å°‘ä¸€ç‚¹ç‚¹ï¼Œæ¯”å¦‚ 0.02
            time.sleep(0.02)

    except Exception as e:
        print(f"æµå¤„ç†é”™è¯¯: {e}")
        traceback.print_exc()
    finally:
        cap.release()

# ===========================
# 4. Flask è·¯ç”±
# ===========================

@app.route('/')
def index():
    # ä¼ é€’å¿…è¦å˜é‡ç»™æ¨¡æ¿
    current_year = datetime.datetime.now().year
    gestures_list = classifier.LABELS if hasattr(classifier, 'LABELS') else []
    return render_template('index.html', gestures=gestures_list, current_year=current_year)

@app.route('/video_feed')
def video_feed():
    return Response(generate_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/toggle_recognition', methods=['POST'])
def toggle_recognition():
    data = request.json
    if 'is_running' in data:
        state.is_running = bool(data['is_running'])
        # å¦‚æœåœæ­¢ï¼Œæ¸…ç©ºä¸€ä¸‹é˜Ÿåˆ—
        if not state.is_running:
            state.raw_predictions_queue.clear()
            state.last_prediction = None
        return jsonify({'status': 'success', 'running': state.is_running})
    return jsonify({'status': 'error'}), 400

@app.route('/api/state', methods=['GET'])
def get_state():
    # å‰ç«¯è½®è¯¢è¿™ä¸ªæ¥å£è·å–æœ€æ–°æ•°æ®
    return jsonify({
        'mode': state.mode,
        'is_running': state.is_running,
        'hand_detected': state.hand_detected,

        # è¿™é‡Œçš„ last_prediction å·²ç»æ˜¯ç»è¿‡ å¹³æ»‘+é˜ˆå€¼è¿‡æ»¤ åçš„ç»“æœäº†
        'last_prediction': state.last_prediction,
        'last_confidence': state.last_confidence,

        'prediction_history': state.prediction_history,
        'gesture_statistics': state.gesture_statistics,
        'performance_metrics': state.performance_metrics,
        'show_landmarks': state.show_landmarks,
        'confidence_threshold': state.confidence_threshold,
        'total_predictions': sum(state.gesture_statistics.values())
    })

@app.route('/api/settings', methods=['POST'])
def update_settings():
    data = request.json
    # æ›´æ–°è®¾ç½®
    if 'confidence_threshold' in data:
        state.confidence_threshold = float(data['confidence_threshold'])
        print(f"é˜ˆå€¼æ›´æ–°ä¸º: {state.confidence_threshold}")

    if 'smooth_window' in data:
        val = int(data['smooth_window'])
        state.smooth_window = max(1, min(10, val))
        # è°ƒæ•´é˜Ÿåˆ—é•¿åº¦
        state.raw_predictions_queue = collections.deque(state.raw_predictions_queue, maxlen=state.smooth_window)

    if 'show_landmarks' in data:
        state.show_landmarks = bool(data['show_landmarks'])

    if 'mode' in data:
        state.mode = data['mode']

    if 'is_recording' in data:
        state.is_recording = bool(data['is_recording'])

    return jsonify({'status': 'success'})

@app.route('/api/clear_history', methods=['POST'])
def clear_history():
    state.prediction_history = []
    return jsonify({'status': 'success'})

@app.route('/api/reset_statistics', methods=['POST'])
def reset_statistics():
    state.gesture_statistics = {}
    return jsonify({'status': 'success'})


# æ‰¾åˆ° web_app.py æœ€åº•éƒ¨çš„è¿™å‡ è¡Œ
if __name__ == '__main__':
    # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
    os.makedirs('templates', exist_ok=True)
    os.makedirs('static', exist_ok=True)

    print("å¯åŠ¨ Web åº”ç”¨...")
    print("è¯·ç¡®ä¿ hand_landmarks.py å’Œ gesture_classifier.py åœ¨åŒä¸€ç›®å½•")

    # ğŸ”´ ä¿®æ”¹è¿™é‡Œï¼š
    # 1. debug=False (å…³é—­è°ƒè¯•æ¨¡å¼ï¼Œé˜²æ­¢é‡è½½å™¨å†²çª)
    # 2. threaded=True (å¼€å¯å¤šçº¿ç¨‹ï¼Œè®©è§†é¢‘æµå’ŒAPIè¯·æ±‚äº’ä¸å¹²æ‰°)
    app.run(host='0.0.0.0', port=5001, debug=False, threaded=True)