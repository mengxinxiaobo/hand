import cv2
import streamlit as st
import numpy as np
import time
# ç¡®ä¿è¿™ä¸¤ä¸ªæ–‡ä»¶åœ¨ä½ çš„ä»“åº“ä¸­å­˜åœ¨
from hand_landmarks import HandLandmarkDetector
# from gesture_classifier import GestureClassifier # å¦‚æžœè¿˜æ²¡ä¸Šä¼ è¿™ä¸ªæ–‡ä»¶ï¼Œè¯·å…ˆæ³¨é‡ŠæŽ‰

# -------------------------------------------------------------------------
# ä¸ºäº†é˜²æ­¢æŠ¥é”™ï¼Œå¦‚æžœä½ è¿˜æ²¡æœ‰ GestureClassifierï¼Œæˆ‘åŠ äº†ä¸€ä¸ªæ¨¡æ‹Ÿç±»ã€‚
# å¦‚æžœä½ å·²ç»ä¸Šä¼ äº† gesture_classifier.pyï¼Œè¯·åˆ é™¤ä¸‹é¢è¿™ä¸ª Mock ç±»ï¼Œ
# å¹¶å–æ¶ˆä¸Šé¢ from gesture_classifier... çš„æ³¨é‡Š
class MockGestureClassifier:
    def __init__(self):
        self.LABELS = ['A', 'B', 'C', 'OK', 'Five']
    def predict(self, features):
        # éšæœºè¿”å›žä¸€ä¸ªç»“æžœç”¨äºŽæµ‹è¯•
        return np.random.choice(self.LABELS), np.random.random()

# è¯·æ ¹æ®å®žé™…æƒ…å†µå†³å®šä½¿ç”¨å“ªä¸€ä¸ª
try:
    from gesture_classifier import GestureClassifier
except ImportError:
    GestureClassifier = MockGestureClassifier
# -------------------------------------------------------------------------

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ±‰è¯­æ‰‹æŒ‡å­—æ¯è¯†åˆ«",
    page_icon="ðŸ‘‹",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ–æ¨¡åž‹ (ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤åŠ è½½)
@st.cache_resource
def init_models():
    detector = HandLandmarkDetector()
    classifier = GestureClassifier()
    return detector, classifier

try:
    detector, classifier = init_models()
except Exception as e:
    st.error(f"æ¨¡åž‹åŠ è½½å¤±è´¥: {e}")
    st.stop()

# åº”ç”¨çŠ¶æ€ç®¡ç† (ä½¿ç”¨ session_state æŒä¹…åŒ–æ•°æ®)
if 'history' not in st.session_state:
    st.session_state.history = []
if 'stats' not in st.session_state:
    st.session_state.stats = {}

# é¡µé¢æ ‡é¢˜
st.title("ðŸ‘‹ æ±‰è¯­æ‰‹æŒ‡å­—æ¯è¯†åˆ« (äº‘ç«¯ç‰ˆ)")

# ä¾§è¾¹æ è®¾ç½®
with st.sidebar:
    st.header("è®¾ç½®")
    
    # æ˜¾ç¤º/éšè—æ‰‹éƒ¨å…³é”®ç‚¹
    show_landmarks = st.toggle("æ˜¾ç¤ºæ‰‹éƒ¨å…³é”®ç‚¹", value=True)
    
    # ç½®ä¿¡åº¦é˜ˆå€¼æ»‘å—
    confidence_threshold = st.slider(
        "ç½®ä¿¡åº¦é˜ˆå€¼",
        min_value=0.1,
        max_value=1.0,
        value=0.6,
        step=0.05
    )
    
    # æ¸…é™¤åŽ†å²æŒ‰é’®
    if st.button("æ¸…é™¤åŽ†å²æ•°æ®"):
        st.session_state.history = []
        st.session_state.stats = {}
        st.rerun()

# ä¸»å†…å®¹åŒº
col1, col2 = st.columns([3, 2])

with col1:
    st.subheader("ðŸ“· æ‘„åƒå¤´è¾“å…¥")
    # æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ st.camera_input æ›¿ä»£ cv2.VideoCapture
    img_file_buffer = st.camera_input("ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®æ‹ç…§è¿›è¡Œè¯†åˆ«")

with col2:
    st.subheader("ðŸ“Š è¯†åˆ«ç»“æžœ")
    result_placeholder = st.empty()
    metrics_placeholder = st.empty()
    stats_placeholder = st.empty()

# å¤„ç†é€»è¾‘
if img_file_buffer is not None:
    start_time = time.time()
    
    # 1. å°†ä¸Šä¼ çš„å›¾ç‰‡è½¬æ¢ä¸º OpenCV æ ¼å¼
    bytes_data = img_file_buffer.getvalue()
    # imdecode è¯»å–çš„æ˜¯ BGR æ ¼å¼
    frame = cv2.imdecode(np.frombuffer(bytes_data, np.uint8), cv2.IMREAD_COLOR)
    
    # 2. é•œåƒç¿»è½¬ (è®©è‡ªæ‹çœ‹èµ·æ¥æ›´è‡ªç„¶)
    frame = cv2.flip(frame, 1)
    
    # 3. æ£€æµ‹å…³é”®ç‚¹
    detection_start = time.time()
    landmarks, annotated_frame = detector.detect(frame)
    detection_time = (time.time() - detection_start) * 1000
    
    prediction = None
    confidence = 0.0
    classification_time = 0
    
    # 4. æ‰‹åŠ¿åˆ†ç±»
    if landmarks is not None:
        features = detector.extract_features(landmarks)
        if features is not None:
            cls_start = time.time()
            prediction, confidence = classifier.predict(features)
            classification_time = (time.time() - cls_start) * 1000
            
            # åªæœ‰ç½®ä¿¡åº¦è¶³å¤Ÿé«˜æ‰è®°å½•
            if confidence > confidence_threshold:
                # æ›´æ–°åŽ†å²å’Œç»Ÿè®¡
                st.session_state.history.append((prediction, confidence))
                # ä¿æŒåŽ†å²è®°å½•åªæœ‰æœ€è¿‘ 20 æ¡
                if len(st.session_state.history) > 20:
                    st.session_state.history.pop(0)
                
                # æ›´æ–°ç»Ÿè®¡
                st.session_state.stats[prediction] = st.session_state.stats.get(prediction, 0) + 1

                # åœ¨å›¾ç‰‡ä¸Šç»˜åˆ¶è‹±æ–‡ç»“æžœ (OpenCVä¸æ”¯æŒä¸­æ–‡)
                cv2.putText(annotated_frame, f"Gesture: {prediction}", (10, 50), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                cv2.putText(annotated_frame, f"Conf: {confidence:.2f}", (10, 90), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

    # 5. æ˜¾ç¤ºå¤„ç†åŽçš„å›¾åƒ
    # å¦‚æžœç”¨æˆ·é€‰æ‹©ä¸æ˜¾ç¤ºå…³é”®ç‚¹ï¼Œå°±ç”¨åŽŸå›¾
    final_image = annotated_frame if show_landmarks else frame
    # OpenCV æ˜¯ BGRï¼ŒStreamlit éœ€è¦ RGB
    final_image = cv2.cvtColor(final_image, cv2.COLOR_BGR2RGB)
    
    # åœ¨å·¦ä¾§åˆ—æ˜¾ç¤ºå¤„ç†åŽçš„å›¾
    with col1:
        st.image(final_image, caption="è¯†åˆ«å¤„ç†è§†å›¾", use_container_width=True)

    # 6. æ˜¾ç¤ºå³ä¾§æ•°æ®é¢æ¿
    with result_placeholder:
        if prediction and confidence > confidence_threshold:
            st.success(f"è¯†åˆ«ç»“æžœ: **{prediction}**")
            st.progress(float(confidence))
        elif landmarks is None:
            st.warning("æœªæ£€æµ‹åˆ°æ‰‹éƒ¨")
        else:
            st.info(f"ç½®ä¿¡åº¦è¿‡ä½Ž (<{confidence_threshold})")

    with metrics_placeholder:
        m1, m2 = st.columns(2)
        m1.metric("æ£€æµ‹è€—æ—¶", f"{detection_time:.1f} ms")
        m2.metric("åˆ†ç±»è€—æ—¶", f"{classification_time:.1f} ms")

    with stats_placeholder:
        st.markdown("### ðŸ“ˆ åŽ†å²ç»Ÿè®¡")
        if st.session_state.stats:
            st.bar_chart(st.session_state.stats)
        
        if st.session_state.history:
            st.markdown("### ðŸ•’ æœ€è¿‘è®°å½•")
            # æ˜¾ç¤ºæœ€è¿‘5æ¡
            for i, (pred, conf) in enumerate(reversed(st.session_state.history[-5:])):
                st.text(f"{i+1}. æ‰‹åŠ¿: {pred} (ç½®ä¿¡åº¦: {conf:.1%})")

else:
    # åˆå§‹çŠ¶æ€æç¤º
    with col1:
        st.info("ðŸ‘‹ è¯·å…è®¸æµè§ˆå™¨ä½¿ç”¨æ‘„åƒå¤´ï¼Œå¹¶ç‚¹å‡» 'Take Photo' æŒ‰é’®å¼€å§‹è¯†åˆ«ã€‚")
