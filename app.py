import cv2
import streamlit as st
import numpy as np
import av
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration
import threading
from collections import deque

# å¯¼å…¥ä½ çš„æ¨¡å‹æ–‡ä»¶
from hand_landmarks import HandLandmarkDetector

# -------------------------------------------------------------
# å…¼å®¹æ€§å¤„ç†ï¼šé˜²æ­¢æ²¡æœ‰ gesture_classifier æ–‡ä»¶å¯¼è‡´æŠ¥é”™
try:
    from gesture_classifier import GestureClassifier
except ImportError:
    # æ¨¡æ‹Ÿåˆ†ç±»å™¨ï¼ˆå ä½ç¬¦ï¼‰
    class GestureClassifier:
        def __init__(self):
            self.LABELS = ['No Model', 'Test']
        def predict(self, features):
            return "Test", 0.0
# -------------------------------------------------------------

# 1. é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ±‰è¯­æ‰‹æŒ‡å­—æ¯è¯†åˆ« (WebRTC)",
    page_icon="ğŸ‘‹",
    layout="wide"
)

# 2. WebRTC é…ç½® (å…³é”®ï¼è¿™æ˜¯äº‘ç«¯ç©¿é€å†…ç½‘å¿…é¡»çš„)
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

# 3. å®šä¹‰è§†é¢‘å¤„ç†å™¨ (WebRTC çš„æ ¸å¿ƒ)
class VideoProcessor(VideoTransformerBase):
    def __init__(self):
        # åˆå§‹åŒ–æ¨¡å‹
        self.detector = HandLandmarkDetector()
        self.classifier = GestureClassifier()
        
        # çŠ¶æ€å‚æ•° (ä»å…¨å±€ session_state æˆ–é»˜è®¤å€¼è·å–)
        self.confidence_threshold = 0.6
        self.smooth_window = 5
        self.show_landmarks = True
        
        # å¹³æ»‘é˜Ÿåˆ—
        self.prediction_queue = deque(maxlen=self.smooth_window)
        self.lock = threading.Lock() # çº¿ç¨‹é”

    def update_params(self, threshold, window, show_lm):
        # ç”¨äºä» UI çº¿ç¨‹æ›´æ–°å‚æ•°
        with self.lock:
            self.confidence_threshold = threshold
            self.smooth_window = window
            self.prediction_queue = deque(maxlen=window)
            self.show_landmarks = show_lm

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        """
        æ¯ä¸€å¸§éƒ½ä¼šåœ¨è¿™ä¸ªå‡½æ•°é‡Œè¢«å¤„ç†
        """
        # 1. å°† av.VideoFrame è½¬ä¸º OpenCV æ ¼å¼
        img = frame.to_ndarray(format="bgr24")
        
        # 2. é•œåƒç¿»è½¬
        img = cv2.flip(img, 1)
        
        # 3. è·å–å½“å‰å‚æ•°
        with self.lock:
            thresh = self.confidence_threshold
            show_lm = self.show_landmarks
        
        # 4. æ‰‹éƒ¨æ£€æµ‹
        landmarks, annotated_frame = self.detector.detect(img)
        
        # å¦‚æœä¸æ˜¾ç¤ºéª¨æ¶ï¼Œå°±ç”¨åŸå›¾
        final_img = annotated_frame if (landmarks is not None and show_lm) else img
        
        # 5. æ‰‹åŠ¿è¯†åˆ«
        if landmarks is not None:
            features = self.detector.extract_features(landmarks)
            if features is not None:
                prediction, confidence = self.classifier.predict(features)
                
                if confidence > thresh:
                    # å¹³æ»‘å¤„ç†
                    self.prediction_queue.append(prediction)
                    
                    # ç»Ÿè®¡çª—å£å†…æœ€é«˜é¢‘çš„æ‰‹åŠ¿
                    if len(self.prediction_queue) >= 1:
                        # ç®€å•çš„æŠ•ç¥¨æœºåˆ¶
                        final_prediction = max(set(self.prediction_queue), key=self.prediction_queue.count)
                        
                        # ç»˜åˆ¶ç»“æœ (OpenCV ä¸æ”¯æŒä¸­æ–‡ï¼Œä½¿ç”¨è‹±æ–‡æ˜¾ç¤º)
                        # ç»¿è‰²è¡¨ç¤ºé«˜ç½®ä¿¡åº¦ï¼Œæ©™è‰²è¡¨ç¤ºä½ç½®ä¿¡åº¦
                        color = (0, 255, 0) if confidence > 0.8 else (0, 165, 255)
                        
                        cv2.putText(final_img, f"Gesture: {final_prediction}", (20, 50), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)
                        cv2.putText(final_img, f"Conf: {confidence:.2f}", (20, 100), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)

        # 6. å°†å¤„ç†åçš„å›¾åƒè½¬å› av.VideoFrame è¿”å›ç»™æµè§ˆå™¨
        return av.VideoFrame.from_ndarray(final_img, format="bgr24")

# -------------------------------------------------------------
# UI ç•Œé¢å¸ƒå±€
# -------------------------------------------------------------

st.title("ğŸ‘‹ æ±‰è¯­æ‰‹æŒ‡å­—æ¯è¯†åˆ« (å®æ—¶æµ)")

# ä¾§è¾¹æ è®¾ç½®
with st.sidebar:
    st.header("è®¾ç½®")
    
    # è·å–ç”¨æˆ·è¾“å…¥
    show_landmarks = st.checkbox("æ˜¾ç¤ºéª¨æ¶", value=True)
    conf_threshold = st.slider("ç½®ä¿¡åº¦é˜ˆå€¼", 0.1, 1.0, 0.6, 0.05)
    smooth_win = st.slider("å¹³æ»‘çª—å£", 1, 10, 5, 1)
    
    st.info("æç¤ºï¼š\n1. é¦–æ¬¡è¿è¡Œè¯·å…è®¸æ‘„åƒå¤´æƒé™\n2. æ‰‹æœºç«¯è¯·ä½¿ç”¨ Chrome/Safari\n3. ç»“æœå°†ç›´æ¥æ˜¾ç¤ºåœ¨è§†é¢‘ä¸Šæ–¹")

# ä¸»ç•Œé¢å¸ƒå±€
col1, col2 = st.columns([0.7, 0.3])

with col1:
    st.subheader("å®æ—¶æ‘„åƒå¤´")
    
    # å¯åŠ¨ WebRTC
    ctx = webrtc_streamer(
        key="hand-gesture",
        mode=webrtc_streamer.WebRtcMode.SENDRECV, # å‘é€å¹¶æ¥æ”¶è§†é¢‘
        rtc_configuration=RTC_CONFIGURATION,      # STUN æœåŠ¡å™¨é…ç½®
        video_processor_factory=VideoProcessor,   # æŒ‡å®šå¤„ç†å™¨
        media_stream_constraints={"video": True, "audio": False},
        async_processing=True,
    )

    # å®æ—¶æ›´æ–°å¤„ç†å™¨å‚æ•°
    if ctx.video_processor:
        ctx.video_processor.update_params(conf_threshold, smooth_win, show_landmarks)

with col2:
    st.subheader("çŠ¶æ€")
    if ctx.state.playing:
        st.success("æ‘„åƒå¤´è¿è¡Œä¸­ âœ…")
        st.write("æ­£åœ¨è¿›è¡Œäº‘ç«¯å®æ—¶æ¨ç†...")
    else:
        st.warning("æ‘„åƒå¤´å·²åœæ­¢ ğŸ›‘")
        st.write("è¯·ç‚¹å‡»å·¦ä¾§ 'START' æŒ‰é’®")
        
    st.markdown("---")
    st.markdown("""
    **å¦‚ä½•ä½¿ç”¨ï¼š**
    1. ç‚¹å‡» **SELECT DEVICE** é€‰æ‹©æ‘„åƒå¤´ã€‚
    2. ç‚¹å‡» **START** å¼€å¯è§†é¢‘æµã€‚
    3. å°†æ‰‹æ”¾å…¥ç”»é¢æ¡†å†…ã€‚
    4. è¯†åˆ«ç»“æœä¼šä»¥ **ç»¿è‰²æ–‡å­—** æ˜¾ç¤ºåœ¨è§†é¢‘å·¦ä¸Šè§’ã€‚
    """)
