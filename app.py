import streamlit as st
import cv2
import numpy as np
import time
from hand_landmarks import HandLandmarkDetector
from gesture_classifier import GestureClassifier
import threading
import queue

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ±‰è¯­æ‰‹æŒ‡å­—æ¯è¯†åˆ«",
    page_icon="ğŸ‘‹",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ–æ£€æµ‹å™¨å’Œåˆ†ç±»å™¨
@st.cache_resource
def init_models():
    detector = HandLandmarkDetector()
    classifier = GestureClassifier()
    return detector, classifier

detector, classifier = init_models()

# åº”ç”¨çŠ¶æ€ç®¡ç†
class AppState:
    def __init__(self):
        self.mode = 'recognition'  # recognition or learning
        self.current_gesture = None
        self.prediction_history = []
        self.last_prediction = None
        self.last_confidence = 0.0
        self.show_landmarks = True
        self.confidence_threshold = 0.6
        self.gesture_statistics = {}
        self.performance_metrics = {
            'frame_rate': 0,
            'detection_time': 0,
            'classification_time': 0
        }
        self.hand_detected = False
        self.smooth_predictions = []
        self.smooth_window = 5
        self.history_size = 20
        self.is_running = False
        self.is_recording = False
        self.demo_gesture = None

state = AppState()

# è§†é¢‘æµå¤„ç†çº¿ç¨‹
class VideoStreamProcessor:
    def __init__(self):
        self.video_queue = queue.Queue(maxsize=10)
        self.running = False
        self.cap = None
    
    def start(self):
        self.running = True
        self.thread = threading.Thread(target=self.process_video)
        self.thread.daemon = True
        self.thread.start()
    
    def stop(self):
        self.running = False
        if self.thread.is_alive():
            self.thread.join(timeout=2)
        if self.cap is not None:
            self.cap.release()
    
    def process_video(self):
        self.cap = cv2.VideoCapture(0)
        if not self.cap.isOpened():
            st.error("æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
            return
        
        frame_count = 0
        start_time = time.time()
        
        while self.running:
            # è®¡ç®—å¸§ç‡
            frame_count += 1
            if frame_count % 30 == 0:
                elapsed = time.time() - start_time
                state.performance_metrics['frame_rate'] = round(30 / elapsed, 1) if elapsed > 0 else 0
                start_time = time.time()
                frame_count = 0
            
            # è¯»å–ä¸€å¸§
            success, frame = self.cap.read()
            if not success:
                break
            
            # æ°´å¹³ç¿»è½¬å›¾åƒï¼ˆé•œåƒæ•ˆæœï¼‰
            frame = cv2.flip(frame, 1)
            
            # æ£€æµ‹æ‰‹éƒ¨å…³é”®ç‚¹
            detection_start = time.time()
            landmarks, annotated_frame = detector.detect(frame)
            state.performance_metrics['detection_time'] = round((time.time() - detection_start) * 1000, 2)
            
            # æ›´æ–°æ‰‹éƒ¨æ£€æµ‹çŠ¶æ€
            state.hand_detected = landmarks is not None
            
            # è¯†åˆ«æ‰‹åŠ¿
            prediction = None
            confidence = 0.0
            
            if landmarks is not None:
                # æå–ç‰¹å¾
                features = detector.extract_features(landmarks)
                
                if features is not None:
                    # é¢„æµ‹æ‰‹åŠ¿
                    classification_start = time.time()
                    prediction, confidence = classifier.predict(features)
                    state.performance_metrics['classification_time'] = round((time.time() - classification_start) * 1000, 2)
                    
                    # ä¿å­˜é¢„æµ‹ç»“æœ
                    state.last_prediction = prediction
                    state.last_confidence = confidence
                    
                    # é¢„æµ‹å¹³æ»‘å¤„ç†
                    state.smooth_predictions.append((prediction, confidence))
                    if len(state.smooth_predictions) > state.smooth_window:
                        state.smooth_predictions.pop(0)
                    
                    # åŸºäºæ»‘åŠ¨çª—å£çš„å¹³æ»‘é¢„æµ‹
                    if len(state.smooth_predictions) >= state.smooth_window:
                        # è®¡ç®—çª—å£å†…çš„é¢„æµ‹ç»Ÿè®¡
                        gesture_counts = {}
                        for gest, conf in state.smooth_predictions:
                            if conf > state.confidence_threshold:
                                gesture_counts[gest] = gesture_counts.get(gest, 0) + 1
                        
                        # é€‰æ‹©æœ€å¸¸è§çš„æ‰‹åŠ¿
                        if gesture_counts:
                            smoothed_prediction = max(gesture_counts, key=gesture_counts.get)
                            
                            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                            state.gesture_statistics[smoothed_prediction] = state.gesture_statistics.get(smoothed_prediction, 0) + 1
                            
                            # æ·»åŠ åˆ°å†å²è®°å½•
                            state.prediction_history.append((smoothed_prediction, confidence))
                            if len(state.prediction_history) > state.history_size:
                                state.prediction_history.pop(0)
            
            # å¦‚æœä¸éœ€è¦æ˜¾ç¤ºå…³é”®ç‚¹ï¼Œä½¿ç”¨åŸå§‹å¸§
            if not state.show_landmarks:
                annotated_frame = frame.copy()
            
            # æ˜¾ç¤ºç»“æœ
            if prediction is not None and confidence > state.confidence_threshold:
                # ä½¿ç”¨å¹³æ»‘åçš„é¢„æµ‹ç»“æœ
                if len(state.smooth_predictions) >= state.smooth_window:
                    gesture_counts = {}
                    for gest, conf in state.smooth_predictions:
                        if conf > state.confidence_threshold:
                            gesture_counts[gest] = gesture_counts.get(gest, 0) + 1
                    
                    if gesture_counts:
                        smoothed_prediction = max(gesture_counts, key=gesture_counts.get)
                        prediction = smoothed_prediction
                
                # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
                text = f"æ‰‹åŠ¿: {prediction}"
                confidence_text = f"ç½®ä¿¡åº¦: {confidence:.1%}"
                
                # æ ¹æ®ç½®ä¿¡åº¦é€‰æ‹©é¢œè‰²
                color = (0, 255, 0) if confidence > 0.7 else (0, 165, 255)
                
                # æ˜¾ç¤ºæ–‡æœ¬
                cv2.putText(annotated_frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
                cv2.putText(annotated_frame, confidence_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
            
            # å°†BGRè½¬æ¢ä¸ºRGB
            rgb_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)
            
            # æ›´æ–°é˜Ÿåˆ—
            if self.video_queue.full():
                self.video_queue.get()
            self.video_queue.put(rgb_frame)

# åˆå§‹åŒ–è§†é¢‘å¤„ç†å™¨
video_processor = VideoStreamProcessor()

# é¡µé¢æ ‡é¢˜
st.title("ğŸ‘‹ æ±‰è¯­æ‰‹æŒ‡å­—æ¯è¯†åˆ«")

# ä¾§è¾¹æ è®¾ç½®
with st.sidebar:
    st.header("è®¾ç½®")
    
    # æ˜¾ç¤º/éšè—æ‰‹éƒ¨å…³é”®ç‚¹
    state.show_landmarks = st.checkbox("æ˜¾ç¤ºæ‰‹éƒ¨å…³é”®ç‚¹", value=True)
    
    # ç½®ä¿¡åº¦é˜ˆå€¼æ»‘å—
    state.confidence_threshold = st.slider(
        "ç½®ä¿¡åº¦é˜ˆå€¼",
        min_value=0.1,
        max_value=1.0,
        value=0.6,
        step=0.05
    )
    
    # å¹³æ»‘çª—å£å¤§å°
    state.smooth_window = st.slider(
        "å¹³æ»‘çª—å£å¤§å°",
        min_value=1,
        max_value=10,
        value=5,
        step=1
    )
    
    # åº”ç”¨æ¨¡å¼
    state.mode = st.radio(
        "åº”ç”¨æ¨¡å¼",
        ['recognition', 'learning'],
        index=0
    )
    
    # å­¦ä¹ æ¨¡å¼ä¸‹çš„æ‰‹åŠ¿é€‰æ‹©
    if state.mode == 'learning':
        state.current_gesture = st.selectbox(
            "é€‰æ‹©è¦å­¦ä¹ çš„æ‰‹åŠ¿",
            classifier.LABELS if hasattr(classifier, 'LABELS') else []
        )
    
    # å¼€å§‹/åœæ­¢æŒ‰é’®
    if st.button("å¼€å§‹è¯†åˆ«", type="primary"):
        if not state.is_running:
            state.is_running = True
            video_processor.start()
    
    if st.button("åœæ­¢è¯†åˆ«"):
        if state.is_running:
            state.is_running = False
            video_processor.stop()
    
    # æ¸…é™¤å†å²æŒ‰é’®
    if st.button("æ¸…é™¤å†å²"):
        state.prediction_history = []
        state.gesture_statistics = {}

# ä¸»å†…å®¹åŒº
col1, col2 = st.columns([3, 2])

with col1:
    st.subheader("å®æ—¶è§†é¢‘æµ")
    video_placeholder = st.empty()
    
with col2:
    st.subheader("è¯†åˆ«ç»“æœ")
    result_placeholder = st.empty()
    
    st.subheader("æ€§èƒ½æŒ‡æ ‡")
    metrics_placeholder = st.empty()
    
    st.subheader("è¯†åˆ«ç»Ÿè®¡")
    stats_placeholder = st.empty()

# ä¸»å¾ªç¯
while True:
    # æ˜¾ç¤ºè§†é¢‘æµ
    if state.is_running and not video_processor.video_queue.empty():
        frame = video_processor.video_queue.get()
        video_placeholder.image(frame, use_column_width=True)
    
    # æ˜¾ç¤ºè¯†åˆ«ç»“æœ
    with result_placeholder:
        if state.last_prediction is not None:
            st.markdown(f"### ğŸ¯ æœ€è¿‘è¯†åˆ«: {state.last_prediction}")
            st.progress(state.last_confidence)
            st.text(f"ç½®ä¿¡åº¦: {state.last_confidence:.1%}")
            
            # æ˜¾ç¤ºé¢„æµ‹å†å²
            if state.prediction_history:
                st.text("\né¢„æµ‹å†å²:")
                history_data = [(i+1, gest, f"{conf:.1%}") for i, (gest, conf) in enumerate(reversed(state.prediction_history[-5:]))]
                st.table(history_data)
        else:
            st.info("ç­‰å¾…æ‰‹åŠ¿è¯†åˆ«...")
    
    # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
    with metrics_placeholder:
        col1, col2, col3 = st.columns(3)
        col1.metric("å¸§ç‡", f"{state.performance_metrics['frame_rate']} FPS")
        col2.metric("æ£€æµ‹æ—¶é—´", f"{state.performance_metrics['detection_time']} ms")
        col3.metric("åˆ†ç±»æ—¶é—´", f"{state.performance_metrics['classification_time']} ms")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    with stats_placeholder:
        if state.gesture_statistics:
            st.bar_chart(state.gesture_statistics)
        else:
            st.info("æš‚æ— ç»Ÿè®¡æ•°æ®")
    
    # æ£€æŸ¥åº”ç”¨æ˜¯å¦åœ¨è¿è¡Œ
    if not state.is_running:
        # æ˜¾ç¤ºå ä½å›¾åƒ
        placeholder_img = np.zeros((480, 640, 3), dtype=np.uint8)
        cv2.putText(placeholder_img, "ç‚¹å‡»å¼€å§‹è¯†åˆ«æŒ‰é’®å¯åŠ¨", (100, 240), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)
        video_placeholder.image(placeholder_img, use_column_width=True)
    
    # ç­‰å¾…ä¸€å°æ®µæ—¶é—´
    time.sleep(0.01)
