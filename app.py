import cv2
import streamlit as st
import numpy as np
import av
import threading
from collections import deque

# 1. ä¿®æ­£å¯¼å…¥ï¼šå•ç‹¬å¯¼å…¥ WebRtcMode å’Œ VideoProcessorBase
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration, WebRtcMode

# å¯¼å…¥ä½ çš„æ¨¡å‹æ–‡ä»¶
from hand_landmarks import HandLandmarkDetector

# -------------------------------------------------------------
# å…¼å®¹æ€§å¤„ç†
try:
    from gesture_classifier import GestureClassifier
except ImportError:
    class GestureClassifier:
        def __init__(self):
            self.LABELS = ['No Model', 'Test']
        def predict(self, features):
            return "Test", 0.0
# -------------------------------------------------------------

st.set_page_config(
    page_title="æ±‰è¯­æ‰‹æŒ‡å­—æ¯è¯†åˆ« (WebRTC)",
    page_icon="ğŸ‘‹",
    layout="wide"
)

RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

# 2. ç±»åä¿®æ­£ï¼šVideoTransformerBase -> VideoProcessorBase
class VideoProcessor(VideoProcessorBase):
    def __init__(self):
        self.detector = HandLandmarkDetector()
        self.classifier = GestureClassifier()
        
        self.confidence_threshold = 0.6
        self.smooth_window = 5
        self.show_landmarks = True
        
        self.prediction_queue = deque(maxlen=self.smooth_window)
        self.lock = threading.Lock()

    def update_params(self, threshold, window, show_lm):
        with self.lock:
            self.confidence_threshold = threshold
            self.smooth_window = window
            self.prediction_queue = deque(maxlen=window)
            self.show_landmarks = show_lm

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        # WebRTC æ¥æ”¶åˆ°çš„å¸§å¤„ç†é€»è¾‘
        img = frame.to_ndarray(format="bgr24")
        
        # é•œåƒç¿»è½¬
        img = cv2.flip(img, 1)
        
        with self.lock:
            thresh = self.confidence_threshold
            show_lm = self.show_landmarks
        
        # æ£€æµ‹
        landmarks, annotated_frame = self.detector.detect(img)
        
        final_img = annotated_frame if (landmarks is not None and show_lm) else img
        
        if landmarks is not None:
            features = self.detector.extract_features(landmarks)
            if features is not None:
                prediction, confidence = self.classifier.predict(features)
                
                if confidence > thresh:
                    self.prediction_queue.append(prediction)
                    
                    if len(self.prediction_queue) >= 1:
                        final_prediction = max(set(self.prediction_queue), key=self.prediction_queue.count)
                        
                        # ç»˜åˆ¶ç»“æœ (åªèƒ½è‹±æ–‡)
                        color = (0, 255, 0) if confidence > 0.8 else (0, 165, 255)
                        cv2.putText(final_img, f"Gesture: {final_prediction}", (20, 50), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)
                        cv2.putText(final_img, f"Conf: {confidence:.2f}", (20, 100), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)

        return av.VideoFrame.from_ndarray(final_img, format="bgr24")

# -------------------------------------------------------------
# UI å¸ƒå±€
# -------------------------------------------------------------

st.title("ğŸ‘‹ æ±‰è¯­æ‰‹æŒ‡å­—æ¯è¯†åˆ« (å®æ—¶æµ)")

with st.sidebar:
    st.header("è®¾ç½®")
    show_landmarks = st.checkbox("æ˜¾ç¤ºéª¨æ¶", value=True)
    conf_threshold = st.slider("ç½®ä¿¡åº¦é˜ˆå€¼", 0.1, 1.0, 0.6, 0.05)
    smooth_win = st.slider("å¹³æ»‘çª—å£", 1, 10, 5, 1)
    st.info("æç¤ºï¼šè¯·å…è®¸æµè§ˆå™¨ä½¿ç”¨æ‘„åƒå¤´æƒé™ã€‚")

col1, col2 = st.columns([0.7, 0.3])

with col1:
    st.subheader("å®æ—¶æ‘„åƒå¤´")
    
    # å¯åŠ¨ WebRTC
    ctx = webrtc_streamer(
        key="hand-gesture",
        # 3. ä¿®æ­£è°ƒç”¨ï¼šç›´æ¥ä½¿ç”¨ WebRtcMode.SENDRECV
        mode=WebRtcMode.SENDRECV, 
        rtc_configuration=RTC_CONFIGURATION,
        video_processor_factory=VideoProcessor,
        media_stream_constraints={"video": True, "audio": False},
        async_processing=True,
    )

    if ctx.video_processor:
        ctx.video_processor.update_params(conf_threshold, smooth_win, show_landmarks)

with col2:
    st.subheader("çŠ¶æ€")
    if ctx.state.playing:
        st.success("æ‘„åƒå¤´è¿è¡Œä¸­ âœ…")
    else:
        st.warning("æ‘„åƒå¤´å·²åœæ­¢ ğŸ›‘")
